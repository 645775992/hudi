package org.apache.spark.sql.execution

import org.apache.spark.{Partition, TaskContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.internal.SQLConf

import scala.reflect.ClassTag

/**
 * NOTE: This is a generalized version of of Spark's [[SQLExecutionRDD]]
 *
 * It is just a wrapper over [[sqlRDD]] which sets and makes effective all the configs from the
 * captured [[SQLConf]]
 *
 * @param sqlRDD the `RDD` generated by the SQL plan
 * @param conf the `SQLConf` to apply to the execution of the SQL plan
 */
class SQLConfInjectingRDD[T: ClassTag](var sqlRDD: RDD[T], @transient conf: SQLConf) extends RDD[T](sqlRDD) {
  private val sqlConfigs = conf.getAllConfs
  private lazy val sqlConfExecutorSide = {
    val newConf = new SQLConf()
    sqlConfigs.foreach { case (k, v) => newConf.setConfString(k, v) }
    newConf
  }

  override val partitioner = firstParent[InternalRow].partitioner

  override def getPartitions: Array[Partition] = firstParent[InternalRow].partitions

  override def compute(split: Partition, context: TaskContext): Iterator[T] = {
    // If we are in the context of a tracked SQL operation, `SQLExecution.EXECUTION_ID_KEY` is set
    // and we have nothing to do here. Otherwise, we use the `SQLConf` captured at the creation of
    // this RDD.
    if (context.getLocalProperty(SQLExecution.EXECUTION_ID_KEY) == null) {
      SQLConf.withExistingConf(sqlConfExecutorSide) {
        firstParent[T].iterator(split, context)
      }
    } else {
      firstParent[T].iterator(split, context)
    }
  }
}